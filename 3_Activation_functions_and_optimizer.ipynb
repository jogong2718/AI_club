{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhOIVvf6LCNgeNVPQ10BnD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jogong2718/AI_club/blob/main/3_Activation_functions_and_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gniaYkvv5RiI"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WwnBBhZ4QYk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJjwmNfM5TsG"
      },
      "source": [
        "Importing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwIp_c3h4VrY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydpv_2ox4W64",
        "outputId": "1e91edf4-068c-4f33-b916-294bc2e3da04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk7b669V5WUi"
      },
      "source": [
        "One-Hot-Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cjmYv8y4YaH"
      },
      "outputs": [],
      "source": [
        "trainingLabelsohe = tf.keras.utils.to_categorical(training_labels)\n",
        "testLabelsohe = tf.keras.utils.to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = +3>New coding method for model structure\n",
        "\n",
        "With past models, we structured them so they looked more visually appealing and easier to understand. Such as:\n",
        "```\n",
        "n = 5\n",
        "model = tf.keras.models.Sequential([\n",
        "tf.keras.layers.Flatten(), \n",
        "tf.keras.layers.Dense(n),\n",
        "tf.keras.layers.Dense(n),\n",
        "tf.keras.layers.Dense(10, activation = \"softmax\")\n",
        "])\n",
        "```\n",
        "For this model, it is optimized for tweaking and adjusting parameters. From what we have learned, it is easy to see the large number of parameters that need to be tweaked for a successful model. \n",
        "\n",
        "We create a function to form a sequential model with the function parameters. The parameters:\n",
        "\n",
        "- len_layers: number of hidden layers\n",
        "- a_dense_list: a list of the number of units for each dense layer\n",
        "- a_dropout_list: a list of the rates for each dropout layer\n",
        "- drop: a boolean for if we want to include dropout layers or not\n",
        "\n",
        "<font size = +3>Activation Functions\n",
        "\n",
        "We have been introduced to one activation function called \"softmax\" used in the last dense layer of the model. A new activation function named \"relu\" is used for the hidden layers. relu is a very common activation function and is effectively a benchmark activation function for most deep learning models. It is prided on how quick it lets models reach their highest accuracy, or how fast the model \"converges\". With how popular the activation function is, it is one of the most simple. With an equation of $$f(x) = max(0,x)$$\n",
        "\n",
        "This esentially means that if input is a number higher than 0, the output is the input, and if the input is less or equal to 0, the output is 0. The graph looks like this:\n",
        "\n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\">\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6hWW40R7zIcG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4Na3MlH4a4V"
      },
      "outputs": [],
      "source": [
        "def run_model(len_layers, a_dense_list, a_dropout_list, drop):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  for i in range(len_layers):\n",
        "    model.add(tf.keras.layers.Dense(a_dense_list[i], activation = \"relu\")) # we use relu for the hidden layers\n",
        "    if drop:\n",
        "      model.add(tf.keras.layers.Dropout(a_dropout_list[i]))\n",
        "  model.add(tf.keras.layers.Dense(10, activation = \"softmax\"))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the function"
      ],
      "metadata": {
        "id": "oDYJPOL44JzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len_layers = 3\n",
        "a_dense_list = [128, 64, 32]\n",
        "a_dropout_list = [0.1, 0.1, 0.1]\n",
        "model = run_model(len_layers, a_dense_list, a_dropout_list, False)"
      ],
      "metadata": {
        "id": "9MaokG6-B1lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ripl9hgE5Y1m"
      },
      "source": [
        "One of the most important topics in machine learning and deep learning is the optimizer, gradient descent, and backpropogation. Now, I just threw 3 big words at you with absolutely no warning. To fully understand the intricacies of these topics, you would need to know linear algebra and multivariable calculus, knowledge that none of us have at this moment. \n",
        "\n",
        "### Optimization\n",
        "\n",
        "An optimizer is an algorithm to search for the most optimal (some minimum value) on a loss surface. Recall the loss surface is how many mistakes the model is making, so it is intuitive that the desired point (the optimal point) is where the loss is small. The location where the loss is smallest (global minima) is the \"best\" parameters we need in our model.\n",
        "\n",
        "Think of the y axis as the loss, and the x axis as the weights or parameters for our model when it is a linear model (third gif). When it is like the first gif, think of the z axis as the loss and the x and y axes as the weights or parameters.\n",
        "\n",
        "<p align='center'>\n",
        "    <img src=\"https://miro.medium.com/max/1400/1*47skUygd3tWf3yB9A10QHg.gif\" width=400></img>\n",
        "</p>\n",
        "\n",
        "Today, there are many different upgrades of optimization algorithm. Please see below:\n",
        "\n",
        "<p align='center'>\n",
        "    <img src=\"https://user-images.githubusercontent.com/11681225/49325458-fc785480-f585-11e8-8d2a-9012d6024c6e.gif\" width=400></img>\n",
        "</p>\n",
        "\n",
        "A 2D representation:\n",
        "\n",
        "<p align='center'>\n",
        "    <img src=\"https://adatis.co.uk/wp-content/uploads/GradientDescentGIF.gif\" width=400></img>\n",
        "</p>\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "Gradient descent optimizers are algorithms used to minimize the loss function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. Essentially, they're a type of optimizer algorithm that uses a bit of nifty calculus, more specifically the derivative, to find the global minima.\n",
        "\n",
        "### Back-Propogation\n",
        "\n",
        "Back-propogation is essentially the application of gradient descent algorithms in each layer of a deep learning model. Essentially, it's gradient descent algorithms on top of gradient descent algorithms.\n",
        "\n",
        "### Modifying the optimizer\n",
        "\n",
        "Now we have a general idea of what the optimizer does, let's learn how to adjust its parameters. \n",
        "\n",
        "- learning rate: This is how big the step of each call of the optomization algorithm. Essentially, it dictates how fast or slow the model converges. Finding the right number for the learning rate is integral to building a good model. \n",
        "\n",
        "This image shows how differing learning rates can affect how the model learns.\n",
        "\n",
        "<p align='center'>\n",
        "    <img src=\"https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\" width=600></img>\n",
        "</p>\n",
        "\n",
        "- beta_1, beta_2, epsilon: You may notice that the names for these parameters are greek letters, this is because they reference variables in the adam equation: \n",
        "\n",
        "\n",
        "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
        "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$ \n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\hat{m}_t = \\dfrac{m_t}{1 - \\beta^t_1}$$\n",
        "$$\\hat{v}_t = \\dfrac{v_t}{1 - \\beta^t_2}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\hat{\\epsilon}} \\hat{m}_t$$\n",
        "\n",
        "link to paper for Adam: https://arxiv.org/pdf/1412.6980.pdf\n",
        "\n",
        "You don't need to know what these equations mean, but know it further adjusts the convergence rate of the model / learning speed. \n",
        "\n",
        "From me just tweaking the learning rate, the model is able to achieve a val_accuracy of 98.41%\n",
        "\n",
        "proof 🙂:\n",
        "\n",
        "![Screen Shot 2023-04-01 at 2.51.44 PM.png](data:image/png;base64,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)\n",
        "\n",
        "Try and further tweak the parameters to surpass this accuracy!\n",
        "\n",
        "P.S, this notebook is **NOT** the 98.41% model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-07,\n",
        ")"
      ],
      "metadata": {
        "id": "7qcKE6zVTtGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVicwQHy4etu"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model.compile(\n",
        "      optimizer = opt,\n",
        "      loss = \"categorical_crossentropy\",\n",
        "      metrics = [\"accuracy\"]\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF4ORR2Q4gll"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit(\n",
        "      training_images, trainingLabelsohe, \n",
        "      validation_data = (test_images, testLabelsohe),\n",
        "      epochs = 50,\n",
        "      batch_size = 20\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The max val_accuracy"
      ],
      "metadata": {
        "id": "4G3gUTZoF-5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YovmvRENlqOe"
      },
      "outputs": [],
      "source": [
        "print(max(history.history[\"val_accuracy\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t6UUsVn6Jb6"
      },
      "source": [
        "Run this cell to plot the results of the model's training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['test_accuracy'])"
      ],
      "metadata": {
        "id": "e6LJA_i1UKIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}