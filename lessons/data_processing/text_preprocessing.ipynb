{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AzrKHGstBIG2",
        "e2aKz02RBN4Q"
      ],
      "mount_file_id": "1LhjQ9aySniE3MG5cQqoH3HMXdBNyIQPb",
      "authorship_tag": "ABX9TyO3JHOtE07vrT+0CMjBFD9D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jogong2718/AI_club/blob/main/text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOaGTSAQrk5r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle as pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import h5py as h5py\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = +5>Reading data csv file"
      ],
      "metadata": {
        "id": "cuWBlYrkdn3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open('data 2.csv', newline='') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  for row in reader:\n",
        "    data.append(row)"
      ],
      "metadata": {
        "id": "3ROO1lQtsOHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "sentiments = []\n",
        "\n",
        "for i in range(1, len(data)):\n",
        "  sentences.append(data[i][0])\n",
        "  sentiments.append(data[i][1])"
      ],
      "metadata": {
        "id": "WnDB6xAAt0fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = +5>Sentiments into numbers\n",
        "\n",
        "<font size = +3>0 : negative\n",
        "\n",
        "<font size = +3>1 : neutral\n",
        "\n",
        "<font size = +3>2 : positive"
      ],
      "metadata": {
        "id": "MDdjuiw2dye0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "change = [\"negative\", \"neutral\", \"positive\"]\n",
        "for i in range(len(sentiments)):\n",
        "  if sentiments[i] in change:\n",
        "    sentiments[i] = change.index(sentiments[i])"
      ],
      "metadata": {
        "id": "jwsD8tI3dbJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('og_sentences.pkl', 'wb') as handle:\n",
        "    pickle.dump(np.asarray(sentences), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Hb9xa_FXKd-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('labels.pkl', 'wb') as handle:\n",
        "    pickle.dump(np.asarray(sentiments), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "6-j2vE9VKmoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further process data (lemmatize and to corpus)"
      ],
      "metadata": {
        "id": "iLvHdvCpKYse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('og_sentences.pkl', 'rb') as handle:\n",
        "    og_sentences = pickle.load(handle)"
      ],
      "metadata": {
        "id": "-nQrpgzpKcbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_sentences = og_sentences.tolist()"
      ],
      "metadata": {
        "id": "6oZswmn8bxKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('labels.pkl', 'rb') as handle:\n",
        "    labels = pickle.load(handle)"
      ],
      "metadata": {
        "id": "yNiSuFqTK-sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_sentences[6:100]"
      ],
      "metadata": {
        "id": "_YbwGaL4PE5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_corpus(a_list):\n",
        "  temp_array = \" \".join(a_list)\n",
        "  temp_array = temp_array.split(\" \")\n",
        "  temp_array = np.unique(temp_array)\n",
        "  temp_array = np.sort(temp_array)\n",
        "  return temp_array"
      ],
      "metadata": {
        "id": "tUwSBBklLDAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_using_corpus(the_sentence):\n",
        "  the_sentence = the_sentence.split(\" \")\n",
        "  sentences_function = np.asarray([np.where(the_sentence[i] == big_corpus)[0][0] for i in range(len(the_sentence))])\n",
        "  return sentences_function"
      ],
      "metadata": {
        "id": "UPDSfz9TLnMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "eliminating the links from sentences"
      ],
      "metadata": {
        "id": "9oImHbPsMaTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random_sentence = \"https://lmao.com why r u like this bruh some other stuff lol\""
      ],
      "metadata": {
        "id": "BA0rUduiOIbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temp_list = random_sentence.split(\" \")\n",
        "# i = 0\n",
        "# while i < len(temp_list):\n",
        "#   if \"http://\" in temp_list[i]:\n",
        "#     temp_list.remove(temp_list[i])\n",
        "#   i+=1\n",
        "# print(\" \".join(temp_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWBa9uscOUcy",
        "outputId": "20b4d708-37a5-4b3e-8ca2-552845610278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why r u like this bruh some other stuff lol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(og_sentences))):\n",
        "  temp_list = og_sentences[i].split(\" \")\n",
        "  ii = 0\n",
        "  while ii < len(temp_list):\n",
        "    if \"http\" in temp_list[ii]:\n",
        "      temp_list.remove(temp_list[ii])\n",
        "    ii+=1\n",
        "  og_sentences[i] = \" \".join(temp_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGfe6gbuMduA",
        "outputId": "0013531c-c8b3-4f44-f8ed-84a57e79250e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5842/5842 [00:00<00:00, 137984.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "og_sentences[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKLVLTbQEc1X",
        "outputId": "ef4a141e-19cc-4767-afac-ee2436163b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .\",\n",
              " '$ESI on lows, down $1.50 to $2.50 BK a real possibility',\n",
              " \"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n",
              " 'According to the Finnish-Russian Chamber of Commerce , all the major construction companies of Finland are operating in Russia .',\n",
              " 'The Swedish buyout firm has sold its remaining 22.4 percent stake , almost eighteen months after taking the company public in Finland .',\n",
              " \"$SPY wouldn't be surprised to see a green close\",\n",
              " \"Shell's $70 Billion BG Deal Meets Shareholder Skepticism\",\n",
              " 'SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANGE RELEASE OCTOBER 14 , 2008 AT 2:45 PM The Company updates its full year outlook and estimates its results to remain at loss for the full year .',\n",
              " \"Kone 's net sales rose by some 14 % year-on-year in the first nine months of 2008 .\",\n",
              " \"The Stockmann department store will have a total floor space of over 8,000 square metres and Stockmann 's investment in the project will have a price tag of about EUR 12 million .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizing"
      ],
      "metadata": {
        "id": "mxMyAl4QdKHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip download nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLwJdotTdaq1",
        "outputId": "42363fec-d07e-451d-e22e-3c94ee3b6107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk\n",
            "  File was already downloaded /content/drive/.shortcut-targets-by-id/1k7z8c600GfOvgCn2q3A2nKaAuEIqkLEH/Jonathan/emotions_project/nltk-3.8.1-py3-none-any.whl\n",
            "Collecting joblib\n",
            "  File was already downloaded /content/drive/.shortcut-targets-by-id/1k7z8c600GfOvgCn2q3A2nKaAuEIqkLEH/Jonathan/emotions_project/joblib-1.2.0-py3-none-any.whl\n",
            "Collecting regex>=2021.8.3\n",
            "  File was already downloaded /content/drive/.shortcut-targets-by-id/1k7z8c600GfOvgCn2q3A2nKaAuEIqkLEH/Jonathan/emotions_project/regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
            "Collecting click\n",
            "  File was already downloaded /content/drive/.shortcut-targets-by-id/1k7z8c600GfOvgCn2q3A2nKaAuEIqkLEH/Jonathan/emotions_project/click-8.1.3-py3-none-any.whl\n",
            "Collecting tqdm\n",
            "  File was already downloaded /content/drive/.shortcut-targets-by-id/1k7z8c600GfOvgCn2q3A2nKaAuEIqkLEH/Jonathan/emotions_project/tqdm-4.65.0-py3-none-any.whl\n",
            "Successfully downloaded nltk regex click joblib tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "7PGw5jDddL24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1a33c5-389d-41fc-fb55-9c3c3c23527f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_word(word):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemma = lemmatizer.lemmatize(word, pos =\"n\")\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "wciSgDjxGnbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatize every word that is a **noun**"
      ],
      "metadata": {
        "id": "A_t-O7xuI4k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(og_sentences))):\n",
        "  temp_list = og_sentences[i].split(\" \")\n",
        "  for ii in range(len(temp_list)):\n",
        "    temp_list[ii] = str(lemmatize_word(temp_list[ii]))\n",
        "  og_sentences[i] = \" \".join(temp_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C29Vwkd7DbxH",
        "outputId": "5e734235-3d78-484b-f377-3d37c3c7f67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5842/5842 [00:00<00:00, 6764.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "og_sentences[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyWLpoK_JBbu",
        "outputId": "10303f6f-a3bf-49b1-8daf-cda70d550b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"The GeoSolutions technology will leverage Benefon 's GPS solution by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .\",\n",
              "       '$ESI on lows, down $1.50 to $2.50 BK a real possibility',\n",
              "       \"For the last quarter of 2010 , Componenta 's net sale doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .\",\n",
              "       'According to the Finnish-Russian Chamber of Commerce , all the major construction company of Finland are operating in Russia .',\n",
              "       'The Swedish buyout firm ha sold it remaining 22.4 percent stake , almost eighteen month after taking the company public in Finland .',\n",
              "       \"$SPY wouldn't be surprised to see a green close\",\n",
              "       \"Shell's $70 Billion BG Deal Meets Shareholder Skepticism\",\n",
              "       'SSH COMMUNICATIONS SECURITY CORP STOCK EXCHANGE RELEASE OCTOBER 14 , 2008 AT 2:45 PM The Company update it full year outlook and estimate it result to remain at loss for the full year .',\n",
              "       \"Kone 's net sale rose by some 14 % year-on-year in the first nine month of 2008 .\",\n",
              "       \"The Stockmann department store will have a total floor space of over 8,000 square metre and Stockmann 's investment in the project will have a price tag of about EUR 12 million .\"],\n",
              "      dtype='<U315')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn data into corpus"
      ],
      "metadata": {
        "id": "XwV8VVoQDti7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "big_corpus = to_corpus(og_sentences)"
      ],
      "metadata": {
        "id": "hrooBwdBLsuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_corpus.tolist()"
      ],
      "metadata": {
        "id": "CsuufpOtMOsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating data from corpus"
      ],
      "metadata": {
        "id": "RgKdftKeef2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(og_sentences))):\n",
        "  og_sentences[i] = sentence_using_corpus(og_sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_Mj0-haegmG",
        "outputId": "de57fe93-92fb-4eb1-dc23-e99e214095d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5842/5842 [00:22<00:00, 264.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train, sentences_test, labels_train, labels_test = train_test_split(og_sentences, labels, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "YPYUIIFm7D1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_train_list = tf.keras.utils.pad_sequences([list(sentences_train[ii]) for ii in range(len(sentences_train))], maxlen=66)\n",
        "input_train_list = np.array(input_train_list)\n",
        "input_train_list.shape\n",
        "np.asarray(input_train_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d6e143-c14c-41cd-a7a3-ae2826cdcb64",
        "id": "vxYMsQpi-wS_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,  9087, 11601,   756],\n",
              "       [    0,     0,     0, ..., 12275, 10139,   756],\n",
              "       [    0,     0,     0, ..., 14176, 10756,   153],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,  3063,  7778,  7166],\n",
              "       [    0,     0,     0, ...,  5545, 13276,   756],\n",
              "       [    0,     0,     0, ...,  9957, 11256, 10990]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_test_list = tf.keras.utils.pad_sequences([list(sentences_test[ii]) for ii in range(len(sentences_test))], maxlen=66)\n",
        "input_test_list = np.array(input_test_list)\n",
        "input_test_list.shape\n",
        "np.asarray(input_test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_-2Zjzt-wTA",
        "outputId": "7924881b-8cb5-4177-f600-cfdba3436df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ..., 10967, 14314,   756],\n",
              "       [    0,     0,     0, ..., 10967, 13971,   756],\n",
              "       [    0,     0,     0, ...,  8570,  9393,   529],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,  2967,   460,   336],\n",
              "       [    0,     0,     0, ..., 10756,  1594,   756],\n",
              "       [    0,     0,     0, ..., 14037, 12100,   756]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_train_list = input_train_list/input_train_list.max()\n",
        "input_test_list = input_test_list/input_test_list.max()"
      ],
      "metadata": {
        "id": "v4l6OhIyCE-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_train = tf.keras.utils.to_categorical(np.asarray(labels_train))"
      ],
      "metadata": {
        "id": "WL7Khwq8BSir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test = tf.keras.utils.to_categorical(np.asarray(labels_test))"
      ],
      "metadata": {
        "id": "Xws1d4fCBbx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sentences_train_adj.pkl', 'wb') as handle:\n",
        "    pickle.dump(np.asarray(input_train_list), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "BG6wdRKe-1yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sentences_test_adj.pkl', 'wb') as handle:\n",
        "    pickle.dump(np.asarray(input_test_list), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "k81yWLHC-7pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_labels_adj.pkl', 'wb') as handle:\n",
        "    pickle.dump(np.asarray(labels_train), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "1bU1R2ZxBiM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test_labels_adj.pkl', 'wb') as handle:\n",
        "    pickle.dump(np.asarray(labels_test), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "qvrOz1pkBiM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
